\documentclass{article}[11pt]
\textheight 8.5in
\usepackage{graphicx}
\usepackage{float}%for forcing position of figs
\usepackage{hyperref}
\usepackage{url}
\usepackage{amsmath}
\usepackage{amssymb}

\begin{document}
\begin{center}
Solving Multi-agent IRL using EM
\end{center}

\section{Summary of Discussions}
In this report we will formalize the problem of associating demonstrated trajectories to agents while inverse learning their reward functions using Expectation-Maximization Algorithm



\section{Problem statement}
A multi-agent system can be described as an MDP with one reward function per agent. We assume no interaction between the agents or that the decisions taken by each agent are independent of each other. In this setup, we are given a set of $n$ demonstrations $\mathcal{D}$, generated by $m$ agents in the system. We assume that we do not have explicit knowledge of which agent generated which demonstration. Our aim is to find the reward function of each agent given these demonstrations using inverse reinforcement learning. We propose to use Expectation-Maximization using the insight that each agent maximizes their own reward function. Hence associating a demonstration to an agent (expectation step) then maximizing this reward will converge.

\section{Aside on Expectation Maximization}

In this section, we will review the formulation and derivation of convergence of EM algorithm \cite{ml}. Assume we obtain samples of data from a distribution parametrized by $\theta$. Assume we have missing information $y$ in the sample and we only observe a part of the data $x$. We also assume that we have access to the prior on the joint likelihood of observed and missing parts of the data i.e, $P(x,y|\theta)$. We would like to maximize the log likelihood of observed data $x$ with respect to the parameter of the distribution $\theta$ since we do not have any information on $y$.
 
\begin{align}
\label{em_objective}
\max_{\theta} \ln p(x|\theta)
\end{align}

EM is used especially when it is hard to optimize the above problem explicitly but it is easier to optimize the joint likelihood $P(x,y|\theta)$. The objective in Eq. \ref{em_objective}, for any probability distribution $q$, can be written as 

\begin{align}
\label{em_trick}
\ln p(x|\theta) &= \int q(y)\ln\frac{p(x,y | \theta)}{q(y)} dy + \int q(y) \ln\frac{q(y)}{p(y|x,\theta)}dy \\
&= \int q(y) \ln \frac{p(x,y)q(y)}{p(y|x,\theta)q(y)}dy\\
&= \int q(y) \ln  p(x|\theta) dy \\
&= \ln  p(x|\theta) \int q(y)dy \\
&= \ln  p(x|\theta)
\end{align}

Hence optimizing Eq. \ref{em_objective} is equivalent to optimizing the RHS of Eq. \ref{em_trick}. 

\begin{align}
\arg \max_\theta \ln p(x|\theta) &= \arg \max_\theta \underbrace{\int q(y)\ln\frac{p(x,y | \theta)}{q(y)} dy }_{\text{Likelihood} = \mathcal{L}(x,\theta)}+ \underbrace{\int q(y) \ln\frac{q(y)}{p(y|x,\theta)}dy}_{\text{KL divergence}}
\end{align} We will use the fact that the KL divergence is a metric which is $0$ only if $q(y) = p(y | x,\theta)$ and $>0$ otherwise for the proof of convergence of EM.

\subsection{The Algorithm}
\textbf{E-step}: Given $\theta^{(t)}$, the value of parameter $\theta$ at iteration $t$,  set $q_t(y) = p(y|x, \theta^{(t)})$. This makes the KL divergence at iteration $t$ go to $0$. The log likelihood is now given by,
\begin{align}
\label{expectation}
\mathcal{L}_t(x,\theta) = \int p(y|x, \theta^{(t)}) \ln p(x,y|\theta) dy - \underbrace{\int p(y|x, \theta^{(t)}) \ln p(y|x, \theta^{(t)}) dy}_{\text{Independent of } \theta}
\end{align}\\
\textbf{M-step}: Set $\theta^{(t+1)} = \arg \max_{\theta} \mathcal{L}_t(x,\theta)$

\subsection{Proof of convergence}
\begin{align*}
\ln p(x|\theta^{(t)}) &= \mathcal{L}(x,\theta^{(t)}) + \underbrace{KL \Big(q_t(y) || p(y | x,\theta^{(t)})\Big)}_{=0\text{ by setting }q_t = p}\\
&= \mathcal{L}_t(x,\theta^{(t)}) \ \ \ \ \ \leftarrow \textbf{E-step}\\
&\le \mathcal{L}_t(x,\theta^{(t+1)}) \ \ \leftarrow \textbf{M-step}\\
&\le \mathcal{L}_t(x,\theta^{(t+1)}) + \underbrace{KL \Big(q_t(y) || p(y | x,\theta^{(t+1)})\Big)}_{> 0\text{ because q }\ne p}\\
&= \mathcal{L}(x,\theta^{(t+1)}) + KL \Big(q_t(y) || p(y | x,\theta^{(t+1)})\Big)\\
&= \ln p(x|\theta^{(t+1)})
\end{align*}
Hence EM guarantees monotonic improvement. 



\section{Method}
In our problem, the data is generated from the distribution $P(\mathcal{D} | \textbf{R})$ where $\textbf{R}$ is the set of all reward functions $\{ \Theta,\Psi,r_3, ..., r_m | r_i:|S \times A| \to \mathbb{R} \ \forall i \}$. Here, the observable data $x$ is the set of all demonstrations $\mathcal{D}$ and the missing data $y$ is the class assignment $c_j$ (comes from $j^{th} agent$) for each demonstration. Our objective is to maximize $P(\mathcal{D} | R)$. However this is a hard problem than maximizing $P(\mathcal{D},cj | R) = P(\mathcal{D} |c_j, R)P(c_j|R)$. This motivates us to formulate the problem of Multi-Agent IRL problem in our setting as EM problem. We assume that the prior on class assignment is given by a distribution $\Psi$ ($P(c_j|R) =P(c_j) = \Psi(c_j)$), which is also assumed to be one of the parameters to be learned. Note that it is common to parametrize the reward function as $r_i = f(\theta_i)$. We denote $\textbf{R}$ in the parametric form as $\Theta = \{\theta_1, \theta_2, ..., \theta_m\}$. The objective of our EM algorithm is 

\begin{align}
\Theta_{opt}, \Psi_{opt} = \arg \max_{\Theta,\Psi} \ln P(\mathcal{D}|\Theta,\Psi)
\end{align}

Assuming that each demonstration was generated independent of the other, we can rewrite the objective as 
\begin{align}
\label{exact}
\Theta_{opt}, \Psi_{opt} &= \arg \max_{\Theta,\Psi} \sum_i \ln( P(\mathcal{D}_i|\Theta,\Psi)) \\
\label{exp}
&= \arg \max_{\Theta,\Psi}\sum_i \sum_j P(c_j|\mathcal{D}_i, \Theta, \Psi) \ln(P(c_j,\mathcal{D}_i|\Theta,\Psi))
\end{align}
We can go from Eq. \ref{exact} to Eq. \ref{exp}  using Eq. \ref{expectation}. The term inside argmax in Eq. \ref{exp} is the expectation step.
Consider 

\begin{align*}
P(c_{j}|\mathcal{D}_i, \Theta,\Psi) \propto P(\mathcal{D}_i| c_{j},\Theta,\Psi)P(c_{j}|\Theta,\Psi)
\end{align*}
Given no knowledge about the number of demonstrations that come from each agent, we can initialize $\Psi$ to the maximum entropy solution to the prior $P(c_{j}|\Theta,\Psi) = \Psi(c_{j})= \frac{1}{m} \ \ \forall i,j $.
The complete expectation step can be written as ,

\begin{align*}
&\sum_i \sum_j \frac{P(\mathcal{D}_i| c_{j},\Theta,\Psi)P(c_{j}|\Theta,\Psi)}{\sum_k P(\mathcal{D}_i| c_{k},\Theta,\Psi)P(c_{k}|\Theta,\Psi)} \ln(P(c_{j},\mathcal{D}_i| \Theta,\Psi))
\end{align*}\\
Substituting the expectation probability term $q_t(c_j)$ from $t^{th}$ iteration (as in Eq. \ref{expectation}) we get the likelihood to be,
\begin{align*}
\mathcal{L}_t(\Theta,\Psi)&=\sum_i \sum_j \frac{P(\mathcal{D}_i| c_{j},\Theta^{(t)},\Psi^{(t)})\Psi^{(t)}(c_{j})}{\sum_k P(\mathcal{D}_i| c_{k},\Theta^{(t)},\Psi^{(t)})\Psi^{(t)}(c_{k})} \ln\big(P(\mathcal{D}_i| c_{j},\Theta,\Psi)\Psi(c_{j})\big)
\end{align*} \\
Let us define 
\begin{align}
\label{def_beta}
\beta_{ij}^{(t)} = \frac{P(\mathcal{D}_i| c_j,\Theta^{(t)},\Psi^{(t)})\Psi^{(t)}(c_{j})}{\sum_k P(\mathcal{D}_i| c_k,\Theta^{(t)},\Psi^{(t)})\Psi^{(t)}(c_{k})}
\end{align} this can be determined from the $t^{th}$ iteration of the EM algorithm. Hence we know the values $\beta_{ij}^{(t)} \ \ \forall i,j$. Plugging these in, the maximization step reduces to 

\begin{align*}
&\Theta^{(t+1)},\Psi^{(t+1)} = \max_{\Theta,\Psi} \sum_i \sum_j \beta_{ij}^{(t)} \ln\big(P(\mathcal{D}_i| c_j,\Theta,\Psi)\Psi(c_{j})\big) \\
&\text{s.t. } \sum_j \Psi(c_{j}) = 1\\
\end{align*}
The objective can be written as, 
\begin{align*}
&\Theta^{(t+1)},\Psi^{(t+1)} = \arg \max_{\Theta,\Psi} \sum_i \sum_j \beta_{ij}^{(t)} \Big[\ln\big(P(\mathcal{D}_i| c_j,\Theta,\Psi)\big) + \ln\big(\Psi(c_{j})\big)\Big]\\
\end{align*}
In the above equation, we note that the first term inside the summation is  $$P(\mathcal{D}_i| c_j,\Theta,\Psi) = P(\mathcal{D}_i| c_j,\Theta) = P(\mathcal{D}_i| c_j,\theta_j) = P(\mathcal{D}_i| \theta_j) $$ because $c_j$ acts just like a switch which selects $j^{th}$ reward function. 

\textbf{Also, the first term in the summation is independent of $\Psi$ and the second term is independent of $\theta$. Hence we can maximize the terms separately as two independent problems. Note that the constraint $\sum_j \Psi(c_j) = 1$ applies to the second term only.  We now have 2 independent problems in M-step.\\ \\}
\textbf{Problem 1: }
\begin{align}
\label{opt_problem}
 \Theta^{(t+1)} = &\sum_j  \arg \max_{\theta_j}\sum_i \beta_{ij}^{(t)} \ln\big(P(\mathcal{D}_i| \theta_j)\big)\\
\end{align} where the inner problem $\mathcal{L}_m(\theta_j)\ = \arg \max_{\theta_j}\sum_i \beta_{ij}^{(t)} \ln\big(P(\mathcal{D}_i| \theta_j)\big)$ is the same as the log likelihood of demonstrations under a single reward function \cite{ziebart2008maximum} except that each likelihood is now weighed by $\beta_{ij}$. The gradient is now given by, 
\begin{align*}
\nabla_{\theta_j}\mathcal{L}_m(\theta_j) = \sum_i \beta_{ij}^{(t)}\tilde{f}_i - \sum_{s_i}\beta_{ij}^{(t)}D_{s_i}f_{s_i}
\end{align*} where $\tilde{f}_i$ is the feature expectation of the $i^{th}$ demonstration and $D_{s_i}$ is the state visitation frequency. Weiging gradients from each term makes sence because, if the probability of a demonstration $i$ coming from agent $j$ is very low $\implies \beta_{ij}^{(t)} \to 0$, then the contribution of the gradient from $i^{th}$ demonstration to $j^{th}$ reward is zero. That is, we do not want to increase the likelihood of $i^{th}$ demonstration by changing the parameters of $j^{th}$ reward as the demonstration could not have come from $j^{th}$ agent anyway. \cite{ziebart2008maximum}. 

According to our assumption that no agents interact, each term under the outer summation in Eq. \ref{opt_problem} is independent maximization problem. Hence our problem will still find the global optimum in deterministic cases and an equivalently optimal solution as MaxEnt IRL solution in stochastic cases. In other words, we do not loose optimality because of this summation.  \\ \\ 
\textbf{Problem 2: }
\begin{align}
&\Psi^{(t+1)} = \arg \max_{\Psi} \sum_i \sum_j \beta_{ij}^{(t)} \ln\big(\Psi(c_{j})\big)\\
\label{constraint}
&\text{s.t. } \sum_k \Psi(c_{k}) = 1
\end{align}

The Lagrangian of Problem 2 is given by,

\begin{align*}
\Gamma &= \sum_i \sum_j \Big[\beta_{ij}^{(t)} \ln\big(\Psi(c_{j})\big)\Big] + \lambda\Big[\sum_k \Psi(c_{k}) - 1\Big]
\end{align*} 
Finding the derivative of the Lagrangian and setting it to zero, we get, 
\begin{align}
\frac{\partial\Gamma}{\partial\Psi(c_{j})} &= \frac{1}{\Psi(c_{j})}\sum_i \beta_{ij} + \lambda = 0\\
(or) \ \ \ \ \ -\lambda  \Psi(c_{j}) &= \sum_i \beta_{ij}\\
\Psi(c_{j}) &\propto \sum_i \beta_{ij}\\
\label{norm}
\Psi(c_{j}) &= \frac{\sum_i \beta_{ij}}{\sum_i \sum_j\beta_{ij}}
\end{align} 
Eq. \ref{norm} comes from the constraint that the probabilities must sum to 1 (Eq. \ref{constraint}). But from Eq. \ref{def_beta}, we see that
\begin{align*}
\sum_j\beta_{ij} &= 1\\
\implies \Psi(c_{j}) &= \frac{\sum_i \beta_{ij}}{\sum_i 1} = \frac{\sum_i \beta_{ij}}{n}
\end{align*} where $n$ is the number of demonstrations. Reintroducing the time index
\begin{align*}
\Psi(c_{j})^{(t+1)} &=  \frac{\sum_i \beta_{ij}^{(t)}}{n}
\end{align*}

\subsection{Dirichlet Process to estimate the number of agents}
In the above problem, consider the case where we do not know the number of agents beforehand. This is a similar to Dirichlet Process Mixture Models \cite{mccullagh2008many}. Our problem exactly fall in the Chinese Restaurant Process category. 
\subsubsection{Aside on DP}
Dirichlet Process is a probability distribution in which every sample is a probability distribution (it is a probability distribution on probability distributions). This is similar to Gaussian Processes in a way (where, every sample point is a function). A Dirichlet process has two parameters: the scaling parameter $\alpha$ and the base distribution $G_0$. Let $G_0$ be the base distribution on the measurable set $X$,then every sample $G$ from $DP(\alpha, G_0)$ is a distribution over discrete points in the set. Every time, we can sample a $G$ from  $DP(\alpha, G_0)$, then sample $x_i$ from G. To find the distribution on the points $x_i$ directly, we can marginalize out the $G$

\begin{align*}
P(x_1, ..., x_N) = \int P(G|\alpha,G_0)\prod_{i=1}^N P(x_i|G)dG
\end{align*}\\ \\
Marginalizing out $G$ introduces dependencies between variables and makes the distribution simple to express,

\[   
x_N|x_1, x_2, ...,x_{N-1} = 
     \begin{cases}
       x_i &\quad\text{with probability} \frac{1}{n-1+\alpha}\\
       \text{new draw from }G_0 &\quad\text{with probability} \frac{\alpha}{n-1+\alpha}
     \end{cases}
\]


\subsubsection{Chinese Restaurant Process}
Given an infinite number of tables in a restaurant, the probability a new customer joins a table $T_j$, for some $j$, is proportional to the number of people sitting in that table. Also, with probability proportional to $\alpha$, a parameter of the distribution, the customer will choose to sit in a new table. Each table $T_j$ is same as the value that each $x_i$ is associated to. Though there are a large number of samples, most of the time, most samples will take same values. 

Let $N-1$ tables be currently occupied, let a new customer choose a table $T_N$.

\begin{align}
\label{dirichlet}
P(T_N = T_i : i \sim [1, 2, .., N-1]| T1,...,T_{N-1}) &= \frac{num(T_i)}{n-1+\alpha}\\
P(T_N = \text{new table} | T1,...,T_{N-1}) &= \frac{\alpha}{n-1+\alpha}
\end{align} where $num(T_i)$ is the number of people in $i^{th}$ table and $n$ is the total number of customers including the new one and $N$ is the total number of tables.

%\begin{align*}
%P(T_1, T_2, ..., T_N) &= P(T_1)P(T_2|T_1)P(T_3| T_1,T_2) ... P(T_N|T1,...,T_{N-1})
%\end{align*} 
%
%Substituting for each term $P(T_j|T1,...,T_{j-1})$ in the above equation from the previous equation, we get
%
%\begin{align}
%\label{dirichlet}
%P(T_1, T_2, ..., T_N) &= \frac{\alpha^N \prod_{j=1}^{N}(
%num(T_j)-1)!}{\alpha (1+\alpha) ... (n-1+\alpha)} \prod_{j=1}^NG_0(T_j)
%\end{align} where $G_0 (T_j)$ is the prior probability of picking that particular new table. The above equation \ref{dirichlet} is a sample from probability distribution which is in-turn sampled from a Dirichlet Process (we marginalize out the probability distribution sampled from DP). 

Our problem is similar to Chinese Restaurant Process except that instead having number individuals sitting in a table, we have probability masses assigned to each table. Let us denote the event of $j^{th}$ class being assigned to a randomly picked demonstration as $c_j$ and the class assignment to every other demonstration as $c_{-j}$\\ \\
We can rewrite the prior as ,

\[   
c_j | c_1, c_2 ,..., c_n = 
     \begin{cases}
       c_k  &\quad\text{with probability } \frac{\sum_{i} \beta_{ik}}{\sum_{i}\sum_j \beta_{ij} +\alpha} = \frac{\sum_{i} \beta_{ik}}{n+\alpha} \\
       \text{new draw from }G_0 &\quad\text{with probability} \frac{\alpha}{n+\alpha}
     \end{cases}
\] 

Note that in the above problem, we consider all $n$ demonstrations to compute the prior as opposed to $n-1$ in Eq. \ref{dirichlet}. We can initialize the clusters with some random assignment if we have some knowledge about the system for faster convergence. Anyway, the Dirichlet prior will converge for any assignment. 

\subsection{Why does DP work. Can we do better?}
Note that we have to fix $\alpha$ which is a parameter of the Dirichlet Process. During the iterations, the number of demonstrations generated by an agent $\sum_{i} \beta_{ij}$ (this will be floating number but analogous to the number of agents) can decrease or increase. Hence if there are a large number of demonstrations associated with an agent, this means a we have higher probability of picking this class than a new class. In the other hand, If we have equal number of demonstration coming from each agent, then picking any agent is equally likely to creating a new agent.

We can also experiment by using unnormalized values $\tilde{\beta}_{ij} = P(\mathcal{D}_i| c_{ij},\Theta^{(t)},\Psi^{(t)})\Psi^{(t)}(c_{j})$ to choose from existing values for cluster assignments. The new probability distribution for cluster assignment becomes, 
\[   
c_j | c_1, c_2 ,..., c_n = 
     \begin{cases}
       c_k  &\quad\text{with probability } \frac{\sum_{i} \tilde{\beta}_{ik}}{\sum_{i}\sum_j \tilde{\beta}_{ij} +\alpha}\\
       \text{new draw from }G_0 &\quad\text{with probability} \frac{\alpha}{\sum_{i}\sum_j \tilde{\beta}_{ij} +\alpha}
     \end{cases}
\] If the unnormalized sum over values of $\beta$ ($\sum_{i}\sum_j \tilde{\beta}_{ij}$) is less, it means that we have lower unnormalized probability of $P(c_j | \mathcal{D})$ which increases the probability of creating a new clas since the existing classes cannot explain the demonstrations (implies that there is another agent who is generating the demonstrations)


\subsection{Practical Issues}
\begin{enumerate}
\item We now have the number agents = $n$ (the number of demonstration. That is we have distribution over $n$ points). This will significantly reduce the speed of computation. One way to alleviate this problem would be to only consider those classes with prior probabilities $\Psi(c_j)$ or likelihood of the class assignment $\beta_{ij}$ comparable to  $\Psi(c_{-j})$ or $\beta_{i,-j}$ respectively. This means that we do not want to consider those classes that contribute very less to the gradient $\nabla_{\theta_j}\mathcal{L}_m(\theta_j)$.
\item How to choose $\alpha$? A large value of $\alpha$ would mean that we encourage creation of new class and a small value would mean otherwise.

\end{enumerate} 

%Implementation note: More efficient way to compute the gradient and the state visitation frequencies is given in the supplement of \cite{levine2011nonlinear}


%\section{Likelihood gradient}
%Here we discuss an alternative to get likelihood gradient more efficiently as given in \cite{levine2011nonlinear}. 

%\begin{align*}
%\mathcal{L_D} &= \sum_i\sum_t\sum_j \theta_{ij}\Big(Q_{s_{i,t},a_{i,t}} - V_{s_{i,t}}\Big)\\\
%&=\sum_i\sum_t\sum_j \theta_{ij}\Big(r_{s_{i,t},a_{i,t}} - V_{s_{i,t}} + \sum_{s'} \gamma T_{s'}^{s_{i,t}a_{i,t}}\Big)
%\end{align*}





\section{Bibliography}

\bibliographystyle{plain}
\bibliography{bibfile}
\end{document}
