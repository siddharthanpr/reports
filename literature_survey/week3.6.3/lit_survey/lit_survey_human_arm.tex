\documentclass{article}[11pt]
\textheight 8.5in
\usepackage{graphicx}
\usepackage{float}%for forcing position of figs
\usepackage{hyperref}
\usepackage{amsmath}

\begin{document}
\begin{center}
Siddharthan Rajasekaran\\
Week of 2/27/2017 - 3/6/2017
\end{center}

\section{Summary of Discussions}
In this report we will summarize methods to predict human swivel angle given the trajectory thus far, picking basis functions to learn motion primitives and demonstrated guided motion planning. We will also discuss about linearly solvable MDPs as its a requirement to understanding Maximum Entropy Inverse Reinforcement Learning. I have also marked some doubts in bold and made some notes and asides when I find something interesting in the paper. I have discussed mainly why only using SVM based IRL is not sufficient, what it is lacking and why we go for MaxEnt IRL. I have documented what we have done for the Tele-Operation of Baxter. I have included a section``Plan for next week". Please correct my plans and let me know if anything else must be included to it. 

\section{Papers read}
\subsection{Spatial Map of Synthesized Criteria for the Redundancy Resolution of Human Arm Movements}
\cite{li2015spatial} starts with an introduction on kinematic redundancy of the human reaching motion. Human arm has 7 degrees of freedom (3 - shoulder, 1 elbow, 3 wrist). However for reaching motion we us only the shoulder and the elbow while the wrist is used to position hands for gripping and other manipulation tasks. The aim is to predic the swivel angle $\phi$ at the next time step given the trajectory until now. Dondor's law, popularly applied to human eyes, was also extended to human arms for the reaching motion to predic the arm motion. However \cite{soechting1995moving} points out the all data collected for human arm movements to satisfy Donder's law were obtained with the arms stretched and that a model that assumes minimizing net work done predicts human arm motions better. Hence predicting human arm motions by minimizing cost functions has been more promising and there has been many such cost functions. In \cite{li2015spatial}, predictions from imposing kinematic constraints and minimizing such costs are taken and a reasonable combination of these functions are shown to be better. The five criteria chosen are:
\begin{enumerate}
\item Maximizing the motion efficiency - the position of the elbow $P_e$ is taken to be on the plane the plane formed by the points $P_s$ (shoulder), $P_w$ (wrist), $P_g$ (goal of the reaching motion)
\item Maintaining the Equilibrium Posture - Given the direction of equilibrium vector passing through the upper arm and the position of the writs, we can get a unique $\phi$. This reduces the actuation of shoulder muscles around the joint
\item Minimizing Joint angle change - Picks the swivel angle $\phi$ that minimizes the l2-norm between the joint angles from this and the next time step. \textbf{Doubt: Won't the minimizing $\phi(k+1)$ be equal to $\phi(k)$ since minimum l2-norm occurs at the same point in joint space?}
\item Minimizing the change in kinetic energy
\item Minimizing total work done in joint space
\end{enumerate} 

Different combinations of these criteria
\begin{enumerate}
\item \textbf{Least squares:} The data of last 20 time steps is chosen from experiment. We assume that the predicted swivel angle is a linear combination of the swivel angle prediction from each criterion. Having taken the data for last 20 time steps, we have 20 equations and five unknowns (each equation predict the next swivel angle as linear function of the weights $c_i$ and the individual criterion predictions). Since one cannot uniquely satisfy such highly constrained problems, we take a least square solution. 
\item \textbf{Maximum entropy} We find the probability distribution over the criteria. The distribution with least information (most randomness) that explains the data is given by maximizing the entropy. Any other distribution with any less entropy means we are assuming more information than that is out there in data. The prediction error of each criterion is given by $\epsilon_i(k) = |\phi_{actual}(k) - \phi_i(k)|$. The constraint on the expectation of standard deviation is given by $\sum_{i=1}^{N}\epsilon_{i}^{2}(k) p_{i}(k) = \hat{\sigma}^{2}(k)$, where  $\hat{\sigma}(k)$ is the shared prediction error (\textbf{Doubt: Is this obtained experimentally? If so how are we computing this at every time-step. If not, then what is the constraint on the expectations of the errors?}). We take the criterion contribution to be a normalized discrete probability distribution according to Maximum Entropy.

\end{enumerate}

The paper then compares the two methods of obtaining the contributions of each criterion. It can be concluded that having a lot of history increases inertia of the component's contribution. This makes it hard to change each coefficient which makes it hard for the algorithm to adapt to if the actual experimental data moves over from minimizing energy to maximizing efficiency or any other criterion. Hence this trade off has to be dealt with in least squares. In case of exponential method it seems this inertia does not hinder the possibility of changing the coefficients as it does not rely on history. Also, exponential method scales linearly with number of criterion, while the least squares requires inverting the matrix which is cubic in the number of criterion (assuming the number of timesteps increases linearly with criterion to yield a matrix of dimensions ($k*d \times d$) for some constant $k$). 

\subsection{Demonstration-Guided Motion Planning (DGMP)}
In \cite{ye2011demonstration}, they introduce DGMP to combine the benefits of learning and planning. Given a set of data of demonstrations, learning algorithms are good at learning task constraints and generalizing to a new performance of the task. However, they lack the ability to change their behavior in new environments. For example, in case of transferring sugar to a bowl using spoon, one has to keep the end effector's yaw constant and facing upward. Though this constraint can be learned, mere execution of learned policy fails in case of new changes in environment that did not exist during the demonstration like obstacles. We can leverage planning algorithms to avoid obstacles while executing the learned behavior. 

To do this, the authors collect several demonstrations of the task. During the task completion, two different types of motion features are recorded 1) Configuration based motion feature in which joint angle trajectory is recorded and 2)Landmark based in which the relative position of the end effector pose w.r.t. landmarks are recorded. The motion feature matrix has each demonstration entered into a row. Each column is the time index during the demonstrations. Each entry $x_{ij}$ in the motion feature matrix is the augmented vector of joint angles and the relative pose of the end effector with respect landmarks. However, these tasks are not aligned in time. To time align them, the authors use Dynamic Time Warping (DTW) which correlates each task with each other looking for similar events in the joint angle trajectory. the similar events are mapped to the same time index while the rest of the trajectory are linearly scaled between the points of high correlation. 

For each time index $j$, one can get the $j^{th}$ column of the motion feature matrix. This corresponds to the pose of the robot at time step $j$ for different  demonstrations. One can find the mean $\mu_j$ and the covariance $\Sigma_j$ of the robot pose at the time step $j$ using these points. Note that during an important even such as dropping the sugar into the bowl, it is required to have the end effector in specific space in the environment. The component in the demonstration that corresponds to the position of end effector w.r.t. the bowl (landmark) will have low variance due to this. Hence it is required during execution that we make sure the end effector is penalized for straying outside low variance regions. One obvious choice of the cost function is weighing the squared error of each component during execution by the inverse of the covariance matrix. Hence cost function $H(\theta) = (\theta - \hat{\theta}_t)^TW^{\theta}_t(\theta - \hat{\theta}_t) + \Sigma_i[(K(\theta) - \hat{x'}_t^{(i)})^TW^{x(i)}_t(K(\theta) - \hat{x'}_t^{(i)}]$ where $K$ is the forward kinematics. 

The above is the learning phase. During execution phase, the authors use MC RRM to plan and avoid obstacles. The MC RRM algorithm takes as input the path resulted by the minimization of a cost metric (such as $H(\theta)$ in the above paragraph). It returns a path avoiding obstacles, while minimizing a cost metric. Initially the guided path is added as nodes to the existing graph if the are in collision or not. Then we check collision for each of these point. If there is any collision, we find the segments of the grasp which are disconnected. We use a bidirectional RRT approach to grow the graph at the periphery of both these segments around the obstacle. A newly sampled point is connected to all nodes in the graph which are at most at distance $d_{max}$ away from the sample. Finally when the segments of the graph are connected we use Dijkstra's algorithm with the same cost metric $H(\theta)$. The sampling is done from the Gaussian with the covariance of the node points as computed in the cost function. This lets samples to be in required region and implicitly following the constraints such as being near the bowl while dropping sugar. The results are tested on a robot and introducing a new obstacle in the arena. The robot successfully transfers sugar to the bowl. 

\subsection{Movement primitives and PCA}
In \cite{lim2005movement} they claim there are two things consistently observed from animal movements 1) Grouping of dofs of limbs 2) Existance of motion primitives are combined to generate complex motions. Also these motion primitives are task dependent. When a task is learned one does not lear the complex trajectory involved in a task bu the constituting motion primitives. These motion primitives are represented using basis functions. In this paper they use PCA to find the principle basis functions that explain most of the data. This restricts the number of basis functions and enhances computational efficiency in real time planning and execution. 

TO do this the paper first defines a dynamic model from which a rich set of motion primitives is derived. Each primitive consists of a finite set of basis functions (to be associated with control law that will stabilize the system). When a desired task goal is input to the planner, we use these primitives and basis function to plan and execute the task. The learning part, given a motion sequence, finds the best scaled and warped motion primitives to best achieve the goal. This can be formulated as an optimization program in which we search over the scaling weights for each movement primitive to achieve the objective (task). THis paper however comes up with a set of basis functions given the motion primitive using PCA on the motion capture data. Since PCA is done one only the sample trajectories though on e may argue that the principle components are linear basis only for the given data, it is expected that these components generalize to all motions with some approximations. 

Data for human arm reaching, pointing, etc... are collected and joint angle trajectories are computed using inverse kinematics (\textbf{Doubt: If we use IK to compute a trajectory, how can we make sure the redundancy is resolved in a similar manner throughout the trajectory. Won't the arm model do unnecessary motions while performing the task?}). The principle components are found in this joint space. This means that the other joint angles are simple some linear functions of these principle components. Thus one can represent the whole trajectory as some linear combination of the basis functions. Given a start and goal in joint space, one can compute the weights in linear combination using trajectory optimization. The cost functions is $J = \int_0^{t_f}||\tau(t)||^2 dt$ where $\tau$ is the torque vector in joint space. This work is compared to optimization using B-splines. Though B splines had lesser cost (optimal), the computational efficiency is less than the PCA method. Hence PCA based method is intended for better computational efficiency. 

\subsection{Linearly Solvable MDPs \cite{todorov2006linearly}}
Consider a system dynamics given by the matrix $\bar{P}$. The elements of the matrix $\bar{p}_{ij}$ denote the probability that the unactuated system will evolve from state $i$ to state $j$. Let the system evolve accrding to the matrix $P(u)$ given an input $u$. Let $P(u) = \bar{P}$ i.e. the unactuated system evolves according to self-dynamics. Now, one can come up with a generalized class of MDP if the control law in this set up is given by $p_{ij}(u) = \bar{p}_{ij}\exp(u_j)$ (the elements of the dynamics matrix under control $u$). THis can be intuitively interpreted as $u_j$ applies an exponentially completing force for the system that the unactuated evolution probability is multiplied by this force. The more $u_j$ exponentially more affinity the system has to go to the state $j$ from any state $i$. One of the constraints on probability is $\sum_j\bar{p}_{ij} = 1$. 

If the system evolves with zero input (according to $P(0)$) one can say that the controller cost is $0$. $P(u)$ for any non zero $u$ will incur a cost and hence there will be a evolution matrix different from $P(0)$. Since $P(u)$ is a monotonically increasing function in $u$, one can compute the controller cost as the distance between $P(u)$ and $P(0)$. Since each row is discrete probability distribution, it makes since to define KL divergence as the distance metric.. $$r(i,u) = KL(p_i)(u) || p_i(0)) = \sum_{j : p_{ij} \ne 0}p_{ij}(u) \ln{\frac{p_{ij}(u)}{p_{ij}(0)}} = \sum_jp_{ij}(u)u_j$$. 

This cost can be interpreted as the weighted sum of probabilities of all place you could go by applying control action $u$. $r$ is the expected pay under the condition that you pay $u_k$ if you successfully transition to state $k$ by applying $u$. 

The cost function for the step is taken as $l(i,u) = q(i) + r(i,u)$, where $q(i)$ could be the penalizing factor for being away from absorbing states ($0$ at absorbing states). By substituting $l(i,u)$, the bellman equation becomes $$v(i) = \min_u[q(i)] + \sum_j\bar{p}_{ij}\exp(u_j)(u_j+v(j))]$$ The constrain for the minimization is $\sum_j p_{ij}\exp(u_j) - 1 = 0$. We can formulate the dual problem with the Lagrangian. The Minimizing control action thus obtained is $$u^*_j(i) = -v(j)-\lambda_i-1$$ $$\lambda_i = \ln\Big(\sum_j\bar{p}_{ij}\exp(-v(j))\Big)-1$$

Here note that $\lambda_i$ is independent of $j$ and this is being subtracted from the cost $u^*_j(i)$. This means that the optimal control law $u^*$ encourages the system to reach good states $i$ which are promising to lead to absorbing states. A promising state will have lower cost ($v(j)$) for all $j$ hence higher value of $\lambda_i$. This means that taking any control actions from state $i$ is discounted! (or that the control law encourages the system to be in $i$ in the first place). 

Under the optimal control the state transition probability becomes, $$p_{ij}(u^*(i)) = \frac{\bar{p}_{ij}\exp(-v(j))}{\sum_k\bar{p}_{ik}\exp(-v(k))}$$ Note that this looks like Boltzmann distribution and $v(j)$ is analogous to $\frac{E}{kT}$. The property of Boltzmann distribution is that the particles in the distribution state at the energy state with maximum randomness.  possible. Aside: Also note how Lyapunov functions are generally energy functions like $x^TSx$ in case of LQR (which is also a locally valid Lyapunov function) and which are taken as value functions (in Dynamic programming based solutions) and here $v(j)$ is analogous to $E$ which has unit energy!

Substituting this optimal control in bellman equation we get, $v(i) = q(i) - \lambda_i - 1$. Let us take $z(i) = \exp(-v(i))$ the Bellman equation simplifies to $z(i) = \exp(-q(i))\sum_i\bar{p}_{ij}z(j)$. We can change the above equation to matrix form by taking $G = Diag\{ \exp(-q(i))\}$. Now we can write, $$\textbf{z}= \textbf{G}\bar{P}\textbf{z}$$. Hence $\textbf{z}$ is an eigen vector of the matrix $\textbf{G}\bar{P}$. Hence, solving the MDP has been reduced to the linear problem of finding the eigen vector. Now, we can use power method to do this. We know that the evolution matrix has each row summing to $1$. Hence the spectral radius of $\bar{P} = 1$. We now weigh each row by $G$ which is leer than $1$. Hence the spectral radius of $G\bar{P}$ is at most $1$. Therefore, the eigen vector corresponding to the eigen value 1 can be found using the power method. $$\textbf{z}_{k+1} = G\bar{P}\textbf{z}_k$$

\subsection{Maximum Entropy IRL \cite{ziebart2008maximum}}
For the linear reward function $R = w^t\phi$, one can find $w$ that maximizes the reward at expert's behavior using the SVM based method proposed by Abbeel et. al. However, finding the low dimensional $w$ to maximize the outcome at expert's policy is an ill posed problem. This means that there can be many $w$ for which this can happen (including the trivial degenerate case when $w = 0$). Also there can be many behaviors other than the ones demonstrated by the expert that maximize the reward function (we actually rely on this for generalization). The problem with SVM based method is that it does not disambiguate these solutions. This degree of freedom can be used to specify another objective function which can be maximized. An appropriate function would be to maximize entropy. That is, we do not assume any more information of the expert's behavior than that provided by his/her demonstrations. The constraint under the maximization is that the expectation over the behaviors should match the expected reward. In SVM based method only the constraint is satisfied and any feasible solution is accepted. This translate to: Don;t commit to any behavior than the constraint wants you to (or) we do not want to falsely prefer any behavior without knowing if the demonstrator intended this behavior over the others. 

Under the maximum entropy model, the distribution over paths $\tau$ is given by $P(\tau|\theta) = \frac{1}{Z(w)}\exp(w^T\phi_\tau)$ where $Z(w)$ is the partition function that normalizes over all possible paths and $\phi_\tau$ is the expectation of feature while traversing $\tau$. The paper gives the above distribution over trajectories  in non-deterministic setting (using an approximation on transition probability that it is constant) and stochastic policies. 

\section{Teleoperation of Baxter}
We find the trajectory of the end effector $P^\mathcal{T}_e(t)$ with respect to torso frame $\{\mathcal{T}\}$ and execute the trajectory in Baxter with respect to it's body frame. In this I am including the method we are computing $P^\mathcal{T}_e(t)$, matching the frequencies and the IK (only a pseudo inverse). 

\subsection{Finding $P^\mathcal{T}_e(t)$}
We attach to the body of the expert, three markers 1) Center of belly $b$ 2) Right shoulder $r$ and 3) Left shoulder $l$. We also attach three markers $ee$, $ee1$ and $ee2$ to the end effector (right hand) of the expert. From the motion capture system, we get the poses of all the markers with respect to the map frame $m$. We wish to attach a frame on the expert similar to that of Baxter for mapping the actions. Hence we take a frame attached to the marker $b$ (belly). The direct of x axis should be along the direction the expert is facing. We can compute this by,

$$\bar{X} = (P^m_l - P^m_b)\times(P^m_r - P^m_b)$$ where, $m$ is the map frame (of mocap system) and $\times$ is the cross product. We take $[0,0,1]$ to be the Z-axis. Note that $\bar{X}$ is not perpendicular to Z-axis. Hence the X-axis is found using the component of $\bar{X}$ along the plane $z=0$, which we can compute by setting the z component of $\bar{X} = 0$. X-axis is then obtained by simply normalizing the resultant direction. Y-axis is the cross product between Z and X axes. Now we have the position and the orientation of the frame attached to the belly with respect to map hence we can compute the homogeneous transform from map to belly frame $T^m_b$. 

\begin{align*}
T^m_b = 
\begin{bmatrix}
X&Y&Z&P^m_b\\
0&0&0&1
\end{bmatrix}
\end{align*}



$$P^b_{ee} = T^b_m P^m_{ee}$$
where $T^b_m = (T^m_b)^{-1}$ and $P^b_{ee}$ 3-D position of the end effector with respect to torso frame which is the required $P^\mathcal{T}_e$. 

\subsection{Filtering and Matching frequencies}
Firstly, we get rid of the noise. We compute the velocity of each marker using the previous position, time and and the current position and time. We took a typical demonstration and computed the average velocities of all the markers. The average was around 1200 mm/s (1.2 m/s). We chose a velocity threshold of about 10 times this value that is 10 m/s to filter out noisy data. This can occur when the marker disappears from one pose and appears at another (maybe due to occlusion). This actually very rarely happened while we were testing but nonetheless it is very dangerous to leave them in the data. 
The frequency at which we receive $P^\mathcal{T}_e$ is 100 Hz. We save a window of length 5 (can be changed using config file) to store $P^\mathcal{T}_e(t_{k-5})$ to $P^\mathcal{T}_e(t_k)$. We compute an average velocity using $v(t_k) = \frac{P^\mathcal{T}_e(t_k)-P^\mathcal{T}_e(t_{k-5})}{t_k-t_{k-5}}$ where $t_k$ is the time at $k^{th}$ time step. 

\subsection{Joint velocities} 
We find the position Jacobian ($J_p$) of Baxter using baxter\_ros package. This is the upper part of the full Jacobian. We take the first three rows of the full Jacobian because we do not care about the angular velocities (we do not want angular velocities to be zero, we just don't care about them) since we would like to just follow the position of the end effector. we compute the joint velocities using the pseudo inverse $\dot{q}(t_k) = (J_P)^{-1}v(t_k)$ where $(J_P)^{-1}$ is the pseudo-inverse. This may not be good since we have only three equations and we need to find seven unknowns. These joint velocities can be direcly executed on Baxter. 
\subsection{Acknowledgements}
Srikanth and Harshil contributed equally to this section



\section{Plan for next week}
As we had discussed, I am planning to do an initial implementation of SVM based IRL \cite{abbeel2004apprenticeship} in a deterministic n-dimensional grid world. I am planning to use a single map and some demonstrations of reaching goal from start (hard-coded or using some planners + noise). I am including the features 1) distance to the goal, 2) distance to obstacles, 3) did we just collide with an obstacle, 4) did we just reach goal and a constant and a discount factor to encourage the agent to collect more reward within a time horizon. Please let me know if I have to include anything in the implementation. 



\section{Bibliography}

\bibliographystyle{plain}
\bibliography{bibfile}
\end{document}
